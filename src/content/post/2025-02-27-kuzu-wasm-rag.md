---
slug: "kuzu-wasm-rag"
title: "Serverless Graph RAG in the Browser with K√πzu-Wasm"
description: "We built an experimental, fully in-browser Graph RAG using K√πzu-Wasm and WebLLM to process LinkedIn data locally. Users can explore a personal knowledge graph and query it with natural language‚Äîwithout any servers. While the results were mixed, this project highlights the potential of local AI-powered graph querying."
pubDate: "Feb 27 2025"
heroImage: "/img/default.png"
categories: ["example"]
authors: ["chang"]
tags: ["kuzu", "cypher", "graph", "rag", "wasm", "llm"]
---

Graph-based retrieval-augmented generation (RAG) has been gaining traction in the AI and database communities, but most implementations require a backend server to process graph queries and return results to an LLM. In this experimental project, we explored whether this process could be made entirely **serverless**, running inside the user‚Äôs browser without external infrastructure.  

Using **K√πzu-Wasm**, the WebAssembly version of the K√πzu graph database, we built a prototype of an in-browser **Graph RAG system**. Users can generate a personal knowledge graph from their **LinkedIn data dump**‚Äîincluding contacts, companies, messages, job positions, and skills‚Äîby simply dragging and dropping their CSV files into the web app. The graph is visualized using **vis.js**, and all query processing happens locally, without relying on a backend server.  

To allow natural language querying, we integrated **WebLLM**, which translates user questions into **Cypher queries**. These queries are executed directly in K√πzu-Wasm inside the browser, and the results are passed back to WebLLM for generating human-readable explanations. This enables a fully **local chatbot** capable of reasoning over personal LinkedIn data while keeping everything private on the user‚Äôs device.  

### Expanding K√πzu‚Äôs AI Integrations  

This experiment builds on our previous **LLM integrations** in the K√πzu ecosystem. We‚Äôve explored using **LangChain** and **LlamaIndex** for enhancing retrieval and reasoning with graph-based data. Additionally, in **K√πzu Explorer**, our web-based UI, we introduced a feature that generates **Cypher queries from natural language prompts**, making it easier for users to interact with their data.  

While the results of this experiment have been mixed, it demonstrates a promising direction for **serverless graph querying** and **local AI-powered knowledge graphs**. In the following sections, we‚Äôll break down how the system works, its challenges, and what we learned from this exploration.  

## Implementation

### Data Modeling and Ingestion  

The schema for our LinkedIn graph database is shown below:  

<Image src="/img/2025-02-27-kuzu-wasm-rag/schema.png" width="600" />

To ingest data into **K√πzu-Wasm**, we follow a multi-step process:  

1. **Upload CSV Files** ‚Äì Users drag and drop their LinkedIn CSV files, which are stored in K√πzu-Wasm‚Äôs **virtual file system**.  
2. **Initial Processing** ‚Äì Using K√πzu‚Äôs `LOAD FROM` feature, the raw CSVs are converted into **JavaScript objects** for further transformation.  
3. **Normalization** ‚Äì In JavaScript, we clean and standardize the data by **fixing timestamps, formatting dates, and resolving inconsistent URIs**.  
4. **Data Insertion** ‚Äì The cleaned data is inserted back into the **K√πzu graph database**, making it queryable.  

The implementation of this process is handled in **`src/utils/LinkedInDataConverter.js`**. The additional normalization step ensures that the LinkedIn dataset is well-structured and ready for querying.

### Graph Visualization  

To help users explore their LinkedIn data, we visualize the graph using **vis.js**. The transformation from database results to a visual graph is straightforward‚Äînodes and relationships are extracted from the database, and we ensure that nodes are **deduplicated** before rendering.  

To enhance readability, we assign **different icons and colors** based on the type of entity. For example, people (contacts and the account owner) get a user icon, companies have a building icon, and skills use a lightbulb icon. This makes the graph more intuitive to navigate.  

The implementation for this visualization is handled in **`src/components/VisualizationView.vue`**. With vis.js, we can easily render the graph and provide interactive features like **zooming, panning, and node highlighting**.

### Natural Language Querying with WebLLM  

To enable **natural language querying**, we integrated **WebLLM**, allowing users to explore their LinkedIn data without manually writing Cypher queries. WebLLM translates user questions into **Cypher queries**, executes them locally in **K√πzu-Wasm**, and generates a **human-readable explanation** of the results.  

#### How It Works  

1. **Query Generation** ‚Äì WebLLM retrieves the **graph schema** from K√πzu and constructs a **query generation prompt**, asking the model to translate the user's natural language question into a **valid Cypher query**.  
2. **Query Execution** ‚Äì The generated Cypher query runs against K√πzu-Wasm.  
3. **Answer Generation** ‚Äì The query results are formatted into structured JSON and fed back to WebLLM with a **QA prompt** to produce a natural language explanation.  

#### Prompt Adaptations  

We adapted the prompts from our **LangChain-K√πzu integration** ([GitHub Repo](https://github.com/kuzudb/langchain-kuzu/)), making a few modifications to improve accuracy and context awareness:  

- **Added Background Information** ‚Äì We provide additional details about the LinkedIn data source in the prompt, helping the LLM better understand the structure and relationships in the dataset.  
- **Schema Representation in YAML** ‚Äì Whenever possible, we represent the schema in **YAML instead of raw JSON**, as LLMs tend to parse and understand YAML more effectively.  


## Results  

We tested our **in-browser Graph RAG system** on a **MacBook Pro 2023 (M2 MAX, 32GB RAM)** using **Llama-3.1-8B-Instruct-q4f32_1-MLC**.  

### **Performance**  
- **Initial model fetch time**: ~300 seconds  
- **Generation speed**:  
  - **Prefill tokens**: 60-70 tokens/sec  
  - **Decode tokens**: 15-20 tokens/sec  
- **Average query generation time**: **10-15 seconds per user input**  

### **Accuracy of Generated Queries**  

For **simple queries**, the model performed well. It correctly generated Cypher queries for straightforward requests such as:  
- *"How many companies did I follow?"*  
- *"Which contacts work at K√πzu, Inc?"*  
- *"Which skills do I have?"*  

(Successful generations are shown below.)  
<Image src="/img/2025-02-27-kuzu-wasm-rag/successful-generations.png">


However, for **more complex queries** requiring **joins, filtering, and aggregation**, the model struggled. It often produced incorrect or incomplete Cypher queries for questions like:  
- *"Who endorsed me the most times?"*  
- *"Which contacts have worked with me at the same company?"*  

## Conclusion  

This experiment demonstrates the feasibility of **fully local, serverless Graph RAG** using **K√πzu-Wasm** and **WebLLM**. While the results are mixed, it highlights both the **advantages** and **challenges** of in-browser AI-powered graph querying.  

### **Pros**  
‚úÖ **Privacy & Confidentiality** ‚Äì All data remains on the user's device, ensuring complete privacy.  
‚úÖ **Zero Installation & Configuration** ‚Äì Runs in the browser with no setup required.  
‚úÖ **Easy Deployment & Maintenance-Free** ‚Äì Can be hosted on any static web service with **no need for database or LLM infrastructure**.  
‚úÖ **Low/Zero Operational Cost** ‚Äì Since computation happens on the client‚Äôs machine, there‚Äôs **no cloud cost** for database queries or LLM calls. A CDN can serve a large number of users efficiently.  

### **Cons**  
‚ùå **Slow Initial Loading** ‚Äì K√πzu-Wasm takes ~10 seconds to load, and WebLLM requires ~300 seconds for the first model load (though subsequent loads are faster).  
‚ùå **Performance Depends on Client‚Äôs Device** ‚Äì Query generation speed and execution depend on the user‚Äôs hardware.  
‚ùå **WebLLM Model Limitations** ‚Äì The **distilled LLM struggles with complex queries**, especially those requiring multiple joins or aggregation.  
‚ùå **Limited Browser Compatibility** ‚Äì Currently, some browsers (e.g., Safari) do not fully support WebLLM or WebGPU standards, making this application unusable on those platforms.

### **Future Potential**  

Looking ahead, **in-browser computing** will continue to improve, making this approach more viable:  
- **Better Web Standards** ‚Äì General Availability of **WebGPU (for faster computation)** and **WASM64 (removing the 4GB memory limit for WebAssembly)** will improve browser performance.  
- **Smaller, Smarter LLMs** ‚Äì **More efficient distilled models** and **fine-tuning for query generation** could significantly improve Cypher query accuracy.  
- **Hardware Advancements** ‚Äì With AI acceleration becoming a focus in modern hardware, **future devices will run LLMs much faster**.  
- **Broader Browser Support** ‚Äì As browsers adopt these standards, compatibility issues will decrease.  

### **Final Thoughts**  

While the current implementation has limitations, **the core idea of fully local, private, and cost-free graph querying is promising**. As web technologies and LLM efficiency improve, an approach like this could become truly practical and widely usable in the future.

## **Try It Yourself**  

We have deployed the **LinkedIn Graph RAG demo** so you can test it in your browser:  

üîó **Live Demo:** [linkedin-wasm-demo.kuzudb.com](https://linkedin-wasm-demo.kuzudb.com)  

You can also explore the **source code** and contribute to the project:  

üìÇ **GitHub Repository:** [github.com/kuzudb/wasm-linkedin-example](https://github.com/kuzudb/wasm-linkedin-example)  

Simply drag and drop your **LinkedIn data dump** into the app and start querying your personal graph‚Äî**all locally, with no servers involved!** üöÄ
