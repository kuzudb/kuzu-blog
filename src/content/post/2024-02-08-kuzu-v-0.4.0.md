---
slug: "kuzu-0.4.0-release"
title: "KÃ¹zu 0.4.0 Release"
description: "Release announcement for KÃ¹zu 0.4.0"
pubDate: "April 29 2024"
heroImage: "/img/default.png"
categories: ["release"]
authors: ["team"]
tags: ["cypher", "extensions", "polars", "pyarrow"]
draft: false
---

With the warmer weather in ðŸ‡¨ðŸ‡¦ approaching, there's cause for excitement on more than one front - we've just released
version **0.4.0** of KÃ¹zu! This is a significant release, as it introduces a new storage layer along with
a host of additional features, improvements and extensions, detailed in this post. Let's gear up!

## Features

### New extensions: DuckDB and PostgreSQL

The first feature we discuss is great news for users who want to move data from external relational databases to KÃ¹zu,
but don't want to do additional ETL using intermediate files. Postgres and DuckDB database extensions are here!

In v0.2.0, we introduced the idea of the KÃ¹zu extensions framework and our first extension, `httpfs`. In
v0.4.0, we are happy to introduce two brand new extensions to connect KÃ¹zu to the following two external databases: DuckDB and PostgreSQL .
For now, these extensions are read-only, allowing you to directly scan data from either database (no write support).

Using them is remarkably simple: You first install the required extension and then load it into your
KÃ¹zu session. The following snippet shows how this is done via the CLI:

```sql
INSTALL <extension_name>;
LOAD EXTENSION <extension_name>;
```

For the external database extensions, `<extension_name>` would be either `duckdb` or `postgres`. The way
this works in KÃ¹zu is demonstrated below in the case of PostgreSQL. You first
attach a Postgres database and its associated tables (while providing the necessary connection string parameters)
to your KÃ¹zu CLI or client session.
Under the hood, KÃ¹zu's cache stores the associated database schema, so your Cypher queries
are aware of the existing tables and their columns.

```sql
ATTACH 'dbname=university user=postgres host=localhost password=yourpassword port=5432' AS db1 (dbtype 'postgres');
```

The following Cypher query attaches a `LOAD FROM` subquery to a `COPY FROM` statement, which is also a new feature
in this release, and is explained in more detail [below](#copy-from-with-subquery). The intent is to
pass the results from scanning the Postgres `person` table to the `COPY FROM` command in KÃ¹zu, so that
you can more easily build graphs from data in external relational databases.

```cypher
CREATE NODE TABLE Person(name STRING, age INT64, PRIMARY KEY (name));
COPY Person FROM (LOAD FROM db1.person RETURN *);
```

At any time you can show the databases (and their types) that are attached to a given KÃ¹zu session.

```cypher
CALL show_attached_databases() RETURN *;
// Output
-----------------------------------
| name            | database type |
-----------------------------------
| db1             | POSTGRES      |
-----------------------------------
| another_db      | DUCKDB        |
-----------------------------------
```

More such extensions are planned in the future to make the process of analyzing graph-like data
obtained from various sources even more seamless.

### Import/Export database

You can now migrate databases between different KÃ¹zu versions without manually writing DDL and `COPY` statements.
In the background, KÃ¹zu generates the required DDL statements to facilitate the transfer of data out
of one version and into another. The `EXPORT DATABASE` command allows you to export the contents of
the database to a specific directory. The query below exports the database to an absolute path,
`/path/to/export`, utilizing the same configuration parameters as `COPY FROM` statements.

```cypher
EXPORT DATABASE TO '/path/to/export' (HEADER=true);
```

Exporting a database generates three Cypher files, one each for the schema, macro definitions and
the `COPY` statements for each table. In addition, the data files are exported to CSV (by default),
or Parquet, if desired.

The `IMPORT DATABASE` command allows you to import the contents of
a database from a specific directory. The query below imports the database from `/path/to/export`.

```cypher
IMPORT DATABASE FROM '/path/to/export';
```

The aim of this functionality is to make it easier to migrate databases between different
KÃ¹zu versions as we continue to evolve the core and the storage layer in future versions, and to
facilitate the sharing of databases between users over longer time periods.

### `COPY FROM` with subquery

In v0.4.0, we added support for using subqueries following the `COPY FROM` statement. This feature allows you to
first perform a task like `MATCH` and then use the results of that query as input to the `COPY FROM` command.

For example, consider that we have a graph with a `User` node label and a `Follows` relationship type.
We want to create a new `Person` node table and a `Knows` relationship table, where the goal is to state that
a `Person` "knows" another `Person` if they follow each other. We can use the `COPY FROM` command with a subquery
to achieve this as follows:

```cypher
// Create node/rel tables
CREATE NODE TABLE Person(name STRING, PRIMARY KEY (name));
CREATE REL TABLE Knows(FROM Person TO Person);
// Run COPY FROM with a subquery
COPY Person FROM (MATCH (a:User) RETURN a.name);
COPY Knows FROM (MATCH (a:User)-[r:Follows]->(b:User) RETURN a.name, b.name);
```

An alternate use case would be when you want to directly scan data from an existing object, such as
a Pandas DataFrame and use the results as input to the `COPY FROM` command THis can be combined with
predicate filters as follows:

```python
# Assumes that you have a KÃ¹zu connection object named `conn`
# Also assumes that you created a node table named `Person` with columns `name` and `age`
import pandas as pd

df = pd.DataFrame({
    "name": ["Adam", "Karissa", "Zhang", "Noura"],
    "age": [30, 40, 50, 25]
})

conn.execute("COPY Person FROM (LOAD FROM df WHERE age < 30 RETURN *")
```

Using subqueries with `COPY FROM` opens up a wider range of possibilities for data manipulation and
transformation prior to inserting data into the database.

### Bulk insert into a non-empty table

In prior releases, the `COPY FROM` command could only be used to bulk insert data into an empty table.
This restriction has now been removed, and in v0.4.0, you can also bulk insert data into a non-empty table,
making it easier to append data to an existing table. This approach is likely much more efficient than
inserting data a record at a time via the `CREATE` command, especially when you have large amounts of data.

Below, we show an example of how `COPY FROM` might be used in conjunction with the subquery feature described
earlier. We have a single table named `Person` for two CSV files that have the same structure.

```cypher
// Create node table
CREATE NODE TABLE Person(name STRING, age INT64, PRIMARY KEY (name));
// Run COPY FROM with a subquery
COPY Person FROM (LOAD FROM "person1.csv" RETURN *);
COPY Person FROM (LOAD FROM "person2.csv" RETURN *);
```

Note that the usual primary key constraints still apply; i.e., if the file `person2.csv` contains a record
whose primary key already exists in the `Person` table, it will produce a `RuntimeError` and the
transaction will be rolled back. From a performance perspective, it's expected that subsequent bulk inserts
could be 2-3x slower than the initial `COPY FROM`, because the database has to perform additional
operations to update existing pages on disk, and updating the node tables' hash indexes or the relationship
table's CSR lists.

### Scan from Pandas PyArrow backend

Earlier versions of KÃ¹zu provided the ability to scan data from a Pandas DataFrame using the NumPy backend.
We are very happy to announce that in v0.4.0, we added support for PyArrow-backed Pandas DataFrames as well.
Make sure to run `pip install -U pyarrow pandas` before trying the example below.

```py
import kuzu
import pandas as pd

db = kuzu.Database("persons")
conn = kuzu.Connection(db)

# Convert the Pandas DataFrame to a PyArrow-backed DataFrame
df = pd.DataFrame({
    "name": ["Adam", "Karissa", "Zhang", "Noura"],
    "age": [30, 40, 50, 25]
}).convert_dtypes(dtype_backend="pyarrow")

# Scan the PyArrow-backed Pandas DataFrame in KÃ¹zu by referencing the DataFrame object
result = conn.execute("LOAD FROM df RETURN *;")
print(result.get_as_df())
```

```bash
      name  age
0     Adam   30
1  Karissa   40
2    Zhang   50
3    Noura   25
```

How does this work under the hood? Internally, KÃ¹zu uses a similar layout to Apache Arrow's `Array`,
allowing it to perform a [memcpy](https://cplusplus.com/reference/cstring/memcpy/) operation,
which is more efficient than a conventional copy.
Using `memcpy` means we directly accesse the values in the memory blocks of the underlying Arrow objects, avoiding the
need to move data from the DataFrame's location in memory.

As Pandas 2.0 evolves, it is adding more and more support for Arrow-backed DataFrames in Python. Using
the PyArrow backend in Pandas offers [numerous benefits](https://pandas.pydata.org/docs/user_guide/pyarrow.html)
over the NumPy backend, including better support for strings and nulls, improved performance and better
interoperability with other full-fledged Arrow-backed DataFrame libraries (like Polars, cuDF, etc.).

### Better integration with Polars

Although this feature came out in a minor release just prior to this one (v0.3.2), it's worth mentioning here.
KÃ¹zu now allows directly outputting the results of a Cypher query as a Polars DataFrame. The query
results are converted to an Arrow table obtained via our `get_as_arrow()`
method, and then seamlessly passed to a Polars DataFrame via the Polars `from_arrow()` method. This feature
was made possible thanks to an [external contributor](https://github.com/kuzudb/kuzu/pull/2985)
via relatively few lines of code, shown below.

```py
import polars as pl

# class QueryResult:
    # ...
    # ...
    def get_as_pl(self) -> pl.DataFrame:
        return pl.from_arrow(data=self.get_as_arrow())
```

With [Apache Arrow](https://arrow.apache.org/) becoming the de facto standard for columnar data interchange in the Python ecosystem, we think
that future versions of KÃ¹zu could benefit from native scanning of Polars DataFrames in Python, in a
similar way to how we now scan PyArrow-backed Pandas DataFrames.

## Performance: Internal ID compression

We now apply compression to the internal IDs of nodes and relationships in the storage layer. This
results in significant reduction in the size of a KÃ¹zu database. For
LDBC SF100, [we observed](https://github.com/kuzudb/kuzu/pull/3116) a **45%** reduction in size for
the `data.kz` file within the KÃ¹zu database directory.

| Version | Size of `data.kz` for LDBC SF100 |
| :---: | :---: |
| 0.3.0 | 126 GB |
| **0.4.0** | **69 GB** |

## Closing Remarks

This post highlighted only a few of the many features and improvements that came along with the v0.4.0 release.
It's recommended to check out our [release notes]() on GitHub for a more comprehensive list.

We are excited to bring these enhancements to the ever-growing KÃ¹zu user community. As always,
our many thanks go out to everyone in the KÃ¹zu team, including our interns and our external contributors
for their excellent work in making this release possible. We encourage you to try out the latest
release on your own workflows and engage with us on [Discord](https://discord.gg/VtX2gw9Rug)!